<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <link rel="icon" type="image/x-icon" href="./media/favicon.png" />
    <title>Week 2: Selection Techniques</title>
    <script type="text/javascript" src="js/main.js"></script>

    <link id="lightStylesheet" rel="stylesheet" href="css/styleCommon.css" />
    <link id="lightStylesheet" rel="stylesheet" href="css/style.css" />
    <link id="darkStylesheet" rel="stylesheet" href="css/styleNight.css" disabled/>
</head>

<body onload="loadPage(4)">
    <header>
    </header>
    <main>
        <div class="main-part">
            <h1>Week 2: Selection Method Types</h1>
            <h2>Overview</h2>
            <p>People have invented many different types of methods to allow VR users to correctly select objects in VR.
                Here, we discuss three selection methods on how they work. They are then classified based on the three
                different selection properties; reach, cardinality, and progressive refinement technique. The three
                selection methods are as follows: GazePuffer, which combines eye movement with cheek puffing to select,
                Look\&MidAir, which combines eye selection and finger selection, and Whirling Interface, which combines
                hand selection with wrist movement.</p>

            <h2>GazePuffer: Hands-Free Input Method Leveraging Puff Cheeks for VR</h2>
            <p>GazePuffer is a method that incorporates the traditional gaze input with cheek puffing, from which it
                gets its compound name. Using their eyes, the user can focus and look at what they want to select.
                Traditionally, this has the problem of the "Midas Touch". This is the problem whereby the VR program is
                unsure if the user wishes to select an object, or is merely observing.</p>
            <p>GazePuffer solves this problem by using the user's mouth gestures. By detecting different ways the user
                puffs their cheeks, different selection methods can be applied. These mappings can be different from
                person to person, or game to game, much like how key-binds for games can differ. Nine different gestures
                were ultimately selected and mapped to different functions. For example, puffing one's cheek out left
                corresponded to imitating a left mouse click.</p>
            <figure>
                <img src="./media/w2/gp1.png" alt="User hangs up phone" title="Hmpf! Goodbye!" />
                <figcaption>In the image, the user shifts his gaze to the phone pop-up window, and then puffs his left
                    cheek to complete the action of ending the call.</figcaption>
            </figure>
            <p>This selection technique uses the eyes to focus on the objects the user wants to select. Thus, it
                inherits its infinite reach and multiple selection properties. Infinite reach, because we are not
                constrained by the physical reach of our arms or legs, and multiple selection, because we can look at
                multiple things at once. Per the figure below, nine different gestures were mapped to different
                functions, making this a discrete, single step way to select objects.</p>
            <figure>
                <img src="./media/w2/gp2.png" alt="Methods to puff cheeks" title="AI training data be like" />
                <figcaption>Drafts illustrating each gesture in the gesture set. Scores of gestures in the final gesture set are highlighted in orange.</figcaption>
            </figure>

            <h2>3D Selection in Mixed Reality: Designing a Two-Phase Technique To Reduce Fatigue</h2>
            <p>Look&MidAir is a concepts developed by Chaffangeon Caillet et al. that seeks to combine using the eyes to select and the movement of the fingers and arms to help with the selection. The user first focuses on the object they want to select, bending their finger by certain amounts to aid with precision. Then, they disambiguate the object selected by touching the object among the list of objects that the program selected during the previous step.</p>
            <figure>
                <img src="./media/w2/gp3.png" alt="Selecting chart guide" title="Like taking the box of toys before picking the favicon" />
                <figcaption>First phase (Look) consists of pre-selecting objects using a cone directed along the gaze of the eye and controlled by micro-gestures. The second phase (Micro) consists of selecting the target by navigating the list of preselected objects in the context of the 3D scene, or if needed outside of its context.</figcaption>
            </figure>
            <p>This selection technique has infinite reach in the first step, using the eyes to select the region where the object is located. It then switches to a screen-space reach in order to allow the user to select the specific object from the list. Only one object can be selected at a time from the list. This is also logically a discrete iterative refinement; we first select a region and then iterate on that region to find the specific object we want to select.</p>

            <h2>Whirling Interface: Hand-based Motion Matching Selection for Small Target on XR Displays</h2>
            <p>The Whirling Interface is a selection technique that combines hand selection with wrist movement, to help select moving objects. The user focuses uses their right hand to point to an object they want to select, with a laser pointer helping them to show what their fingers are pointing to. Then, they move their wrist and arm to mimic the motion of the moving object to select it.</p>
            <figure>
                <img src="./media/w2/gp4.png" alt="Selection by movement" title="To the left! Take it back now y'allow!" />
                <figcaption>The Whirling Interface enables users to make selections by synchronizing their hand motion with the orbiting movement of targets</figcaption>
            </figure>
            <p>Using a laser pointer allows for an infinite range, but only one object can be selected at a time. By mimicking the motion of the object, this indicates a discrete iterative way to refine the selection.</p>
        </div>
    </main>
    <footer>
    </footer>
</body>

</html>